---
title: "Quickstart"
description: "Get up and running with Avaliar in 5 minutes"
icon: "rocket"
---

Follow these steps to install the Avaliar SDK, trace your first LLM call, and enable safety detection.

<Steps>
  <Step title="Install the SDK">
    Install the Avaliar Python SDK from PyPI.

    ```bash
    pip install avaliar-python-sdk
    ```
  </Step>

  <Step title="Get Your API Key">
    1. Open the [Avaliar Dashboard](https://app.avaliar.ai).
    2. Navigate to **Settings** > **API Keys**.
    3. Click **Create Key** and select the `sdk` scope.
    4. Copy the generated key.

    <Warning>
      Store your API key securely. It will only be displayed once.
    </Warning>
  </Step>

  <Step title="Set Environment Variable">
    Export your API key so the SDK can authenticate automatically.

    <CodeGroup>
      ```bash macOS / Linux
      export AVALIAR_API_KEY="your-key"
      ```

      ```powershell Windows (PowerShell)
      $env:AVALIAR_API_KEY = "your-key"
      ```
    </CodeGroup>

    <Tip>
      Add this to your shell profile (`.bashrc`, `.zshrc`, etc.) so it persists across sessions.
    </Tip>
  </Step>

  <Step title="Add Tracing">
    Wrap any LLM call with the `@traceable` decorator. The SDK automatically captures the input, output, model, provider, latency, and token usage.

    ```python
    from avaliar import traceable
    from openai import AsyncOpenAI

    client = AsyncOpenAI()

    @traceable("llm", model="gpt-4o", provider="openai")
    async def generate(prompt: str) -> str:
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    ```

    Every call to `generate()` now creates a trace in Avaliar with full metadata.
  </Step>

  <Step title="Enable Detection">
    Turn on safety detection by adding `detection=True` and listing the detectors you want to run.

    ```python
    from avaliar import traceable
    from openai import AsyncOpenAI

    client = AsyncOpenAI()

    @traceable(
        "llm",
        model="gpt-4o",
        provider="openai",
        detection=True,
        detectors=["prompt_injection", "jailbreak", "toxicity", "pii", "bias", "hallucination"]
    )
    async def generate(prompt: str) -> str:
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    ```

    <Info>
      Each detector runs independently. You can enable as many or as few as you need. See the [Detection overview](/detection/overview) for details on each detector.
    </Info>
  </Step>

  <Step title="View in Dashboard">
    Open [app.avaliar.ai/traces](https://app.avaliar.ai/traces) to see your traces, detected issues, and latency metrics in real time.

    <Note>
      Traces typically appear in the dashboard within a few seconds of the LLM call completing.
    </Note>
  </Step>
</Steps>

## Next Steps

<CardGroup cols={3}>
  <Card title="@traceable Decorator" icon="at" href="/sdk/traceable">
    Explore all decorator options including span types, custom metadata, and nested traces.
  </Card>
  <Card title="Detection" icon="shield-halved" href="/detection/overview">
    Configure detectors, choose between local and cloud modes, and set severity thresholds.
  </Card>
  <Card title="Benchmarks & Evals" icon="flask-vial" href="/benchmarks/overview">
    Run standardized evaluations to measure model safety and capability.
  </Card>
</CardGroup>
